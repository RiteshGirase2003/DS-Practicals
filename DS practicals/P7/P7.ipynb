{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb38e971",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d49300f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download NLTK resources (if not already downloaded)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74e6d929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the sample sentence\n",
    "sentence = \"I am currently reading a book about the importance of learning, even though it's a bit challenging with all the distractions and numbers of pages.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54207087",
   "metadata": {},
   "source": [
    "Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09d6a2c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization: ['I', 'am', 'currently', 'reading', 'a', 'book', 'about', 'the', 'importance', 'of', 'learning', ',', 'even', 'though', 'it', \"'s\", 'a', 'bit', 'challenging', 'with', 'all', 'the', 'distractions', 'and', 'numbers', 'of', 'pages', '.']\n"
     ]
    }
   ],
   "source": [
    "# Tokenization\n",
    "tokens = word_tokenize(sentence)\n",
    "print(\"Tokenization:\", tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b67669",
   "metadata": {},
   "source": [
    "POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da6620f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS Tagging: [('I', 'PRP'), ('am', 'VBP'), ('currently', 'RB'), ('reading', 'VBG'), ('a', 'DT'), ('book', 'NN'), ('about', 'IN'), ('the', 'DT'), ('importance', 'NN'), ('of', 'IN'), ('learning', 'NN'), (',', ','), ('even', 'RB'), ('though', 'IN'), ('it', 'PRP'), (\"'s\", 'VBZ'), ('a', 'DT'), ('bit', 'NN'), ('challenging', 'VBG'), ('with', 'IN'), ('all', 'PDT'), ('the', 'DT'), ('distractions', 'NNS'), ('and', 'CC'), ('numbers', 'NNS'), ('of', 'IN'), ('pages', 'NNS'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# POS Tagging\n",
    "pos_tags = pos_tag(tokens)\n",
    "print(\"POS Tagging:\", pos_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8d075d",
   "metadata": {},
   "source": [
    "stop words removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d5af030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Tokens (after stop words removal): ['currently', 'reading', 'book', 'importance', 'learning', ',', 'even', 'though', \"'s\", 'bit', 'challenging', 'distractions', 'numbers', 'pages', '.']\n"
     ]
    }
   ],
   "source": [
    "# Stop Words Removal\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "print(\"Filtered Tokens (after stop words removal):\", filtered_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd79b8e",
   "metadata": {},
   "source": [
    "Stemming "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56e70589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed Tokens: ['current', 'read', 'book', 'import', 'learn', ',', 'even', 'though', \"'s\", 'bit', 'challeng', 'distract', 'number', 'page', '.']\n"
     ]
    }
   ],
   "source": [
    "# Stemming\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]\n",
    "print(\"Stemmed Tokens:\", stemmed_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1270bb8",
   "metadata": {},
   "source": [
    "Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6d78aa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized Tokens: ['currently', 'reading', 'book', 'importance', 'learning', ',', 'even', 'though', \"'s\", 'bit', 'challenging', 'distraction', 'number', 'page', '.']\n"
     ]
    }
   ],
   "source": [
    "# Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
    "print(\"Lemmatized Tokens:\", lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d66cd2",
   "metadata": {},
   "source": [
    "2 -> Create representation of documents by calculating Term Frequency and Inverse DocumentFrequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28e2a3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cbd24764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the documents\n",
    "documents = [\n",
    "    \"I am currently reading a book about the importance of learning, even though it's a bit challenging with all the distractions and numbers of pages.\",\n",
    "    \"The quick brown fox jumped over the lazy dog.\",\n",
    "    \"Dogs are loyal companions.\",\n",
    "    \"The cat sat on the mat.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "630a6051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the documents using TF-IDF Vectorizer\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
    "\n",
    "# Get the feature names (terms)\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d4d7cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF representation for Document 1:\n",
      "about: 0.2057\n",
      "all: 0.2057\n",
      "am: 0.2057\n",
      "and: 0.2057\n",
      "bit: 0.2057\n",
      "book: 0.2057\n",
      "challenging: 0.2057\n",
      "currently: 0.2057\n",
      "distractions: 0.2057\n",
      "even: 0.2057\n",
      "importance: 0.2057\n",
      "it: 0.2057\n",
      "learning: 0.2057\n",
      "numbers: 0.2057\n",
      "of: 0.4114\n",
      "pages: 0.2057\n",
      "reading: 0.2057\n",
      "the: 0.2626\n",
      "though: 0.2057\n",
      "with: 0.2057\n",
      "\n",
      "TF-IDF representation for Document 2:\n",
      "brown: 0.3404\n",
      "dog: 0.3404\n",
      "fox: 0.3404\n",
      "jumped: 0.3404\n",
      "lazy: 0.3404\n",
      "over: 0.3404\n",
      "quick: 0.3404\n",
      "the: 0.4346\n",
      "\n",
      "TF-IDF representation for Document 3:\n",
      "are: 0.5000\n",
      "companions: 0.5000\n",
      "dogs: 0.5000\n",
      "loyal: 0.5000\n",
      "\n",
      "TF-IDF representation for Document 4:\n",
      "cat: 0.4215\n",
      "mat: 0.4215\n",
      "on: 0.4215\n",
      "sat: 0.4215\n",
      "the: 0.5380\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the TF-IDF representation of documents\n",
    "for doc_index, doc in enumerate(documents):\n",
    "    print(f\"TF-IDF representation for Document {doc_index + 1}:\")\n",
    "    for term_index, term in enumerate(feature_names):\n",
    "        tfidf_value = tfidf_matrix[doc_index, term_index]\n",
    "        if tfidf_value > 0:\n",
    "            print(f\"{term}: {tfidf_value:.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "128492d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.20571759 0.20571759 0.20571759 0.20571759 0.         0.20571759\n",
      "  0.20571759 0.         0.         0.20571759 0.         0.20571759\n",
      "  0.20571759 0.         0.         0.20571759 0.         0.20571759\n",
      "  0.20571759 0.         0.         0.20571759 0.         0.\n",
      "  0.20571759 0.41143519 0.         0.         0.20571759 0.\n",
      "  0.20571759 0.         0.26261375 0.20571759 0.20571759]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.34041103 0.         0.         0.         0.\n",
      "  0.         0.34041103 0.         0.         0.34041103 0.\n",
      "  0.         0.34041103 0.34041103 0.         0.         0.\n",
      "  0.         0.         0.         0.34041103 0.         0.34041103\n",
      "  0.         0.         0.4345599  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.5        0.\n",
      "  0.         0.         0.         0.         0.5        0.\n",
      "  0.         0.         0.5        0.         0.         0.\n",
      "  0.         0.         0.         0.         0.5        0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.42146317 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.42146317\n",
      "  0.         0.         0.42146317 0.         0.         0.\n",
      "  0.         0.42146317 0.53802897 0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# optional to above code \n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
    "print(tfidf_matrix.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a214dff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 3)\t0.5\n",
      "  (0, 2)\t0.5\n",
      "  (0, 1)\t0.5\n",
      "  (0, 0)\t0.5\n",
      "['am' 'currently' 'doing' 'practicals']\n",
      "TF-IDF representation:\n",
      "{'am': 0.5, 'currently': 0.5, 'doing': 0.5, 'practicals': 0.5}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Define the sentence\n",
    "sentence = \"I am currently doing practicals I\"\n",
    "\n",
    "# Initialize the TF-IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the sentence to calculate TF-IDF\n",
    "tfidf_representation = tfidf_vectorizer.fit_transform([sentence])\n",
    "print(tfidf_representation)\n",
    "\n",
    "# Get the feature names (words) and their corresponding TF-IDF values\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "print(feature_names)\n",
    "\n",
    "# Create a dictionary to store TF-IDF values\n",
    "tfidf_dict = {}\n",
    "for i, feature in enumerate(feature_names):\n",
    "    tfidf_dict[feature] = tfidf_representation[0, i]\n",
    "\n",
    "# Print the TF-IDF representation\n",
    "print(\"TF-IDF representation:\")\n",
    "print(tfidf_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436fd871",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52970f17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
